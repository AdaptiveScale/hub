{
  "parents": [
    "system:cdap-data-streams[4.3.0-SNAPSHOT,6.0.0-SNAPSHOT)",
    "system:cdap-data-pipeline[4.3.0-SNAPSHOT,6.0.0-SNAPSHOT)"
  ],
  "properties": {
    "widgets.Kafka-batchsink": "{\"metadata\":{\"spec-version\":\"1.5\"},\"display-name\":\"Kafka Producer\",\"configuration-groups\":[{\"label\":\"Kafka Producer and Topic Config\",\"properties\":[{\"widget-type\":\"textbox\",\"label\":\"Reference Name\",\"name\":\"referenceName\"},{\"widget-type\":\"csv\",\"label\":\"Kafka Brokers\",\"name\":\"brokers\",\"widget-attributes\":{\"delimiter\":\",\"}},{\"widget-type\":\"textbox\",\"label\":\"Kafka Topic\",\"name\":\"topic\"},{\"widget-type\":\"select\",\"label\":\"Is Async ?\",\"name\":\"async\",\"widget-attributes\":{\"values\":[\"TRUE\",\"FALSE\"],\"default\":\"FALSE\"}},{\"widget-type\":\"select\",\"label\":\"Compression type\",\"name\":\"compressionType\",\"widget-attributes\":{\"values\":[\"none\",\"gzip\",\"snappy\"],\"default\":\"none\"}},{\"widget-type\":\"keyvalue\",\"label\":\"Additional Kafka Producer Properties\",\"name\":\"kafkaProperties\",\"widget-attributes\":{\"showDelimiter\":\"false\",\"key-placeholder\":\"Kafka producer property\",\"value-placeholder\":\"Kafka producer property value\"}}]},{\"label\":\"Message Configuration\",\"properties\":[{\"widget-type\":\"select\",\"label\":\"Message Format\",\"name\":\"format\",\"widget-attributes\":{\"values\":[\"CSV\",\"JSON\"],\"default\":\"CSV\"}},{\"widget-type\":\"textbox\",\"label\":\"Message Key field\",\"name\":\"key\"}]},{\"label\":\"Authentication\",\"properties\":[{\"widget-type\":\"textbox\",\"label\":\"Kerberos Principal\",\"name\":\"principal\"},{\"widget-type\":\"textbox\",\"label\":\"Keytab Location\",\"name\":\"keytabLocation\"}]}],\"outputs\":[]}",
    "widgets.Kafka-batchsource": "{\"metadata\":{\"spec-version\":\"1.5\"},\"display-name\":\"Kafka Consumer\",\"configuration-groups\":[{\"label\":\"Kafka Configuration\",\"properties\":[{\"widget-type\":\"textbox\",\"label\":\"Reference Name\",\"name\":\"referenceName\"},{\"widget-type\":\"csv\",\"label\":\"Kafka Brokers\",\"name\":\"kafkaBrokers\",\"widget-attributes\":{\"delimiter\":\",\"}},{\"widget-type\":\"textbox\",\"label\":\"Kafka Topic\",\"name\":\"topic\"},{\"widget-type\":\"textbox\",\"label\":\"Offset Table Name\",\"name\":\"tableName\"},{\"widget-type\":\"csv\",\"label\":\"Topic Partitions\",\"name\":\"partitions\",\"widget-attributes\":{\"delimiter\":\",\"}},{\"widget-type\":\"keyvalue\",\"label\":\"Initial Offsets\",\"name\":\"initialPartitionOffsets\",\"widget-attributes\":{\"showDelimiter\":\"false\",\"key-placeholder\":\"Partition\",\"value-placeholder\":\"Offset\"}},{\"widget-type\":\"textbox\",\"label\":\"Key Field\",\"name\":\"keyField\"},{\"widget-type\":\"textbox\",\"label\":\"Partition Field\",\"name\":\"partitionField\"},{\"widget-type\":\"textbox\",\"label\":\"Offset Field\",\"name\":\"offsetField\"},{\"widget-type\":\"textbox\",\"label\":\"Max Number Records\",\"name\":\"maxNumberRecords\"},{\"widget-type\":\"keyvalue\",\"label\":\"Additional Kafka Consumer Properties\",\"name\":\"kafkaProperties\",\"widget-attributes\":{\"showDelimiter\":\"false\",\"key-placeholder\":\"Kafka consumer property\",\"value-placeholder\":\"Kafka consumer property value\"}}]},{\"label\":\"Format\",\"properties\":[{\"widget-type\":\"select\",\"label\":\"Format\",\"name\":\"format\",\"widget-attributes\":{\"values\":[\"\",\"avro\",\"binary\",\"clf\",\"csv\",\"grok\",\"syslog\",\"text\",\"tsv\"],\"default\":\"\"}}]},{\"label\":\"Authentication\",\"properties\":[{\"widget-type\":\"textbox\",\"label\":\"Kerberos Principal\",\"name\":\"principal\"},{\"widget-type\":\"textbox\",\"label\":\"Keytab Location\",\"name\":\"keytabLocation\"}]}],\"outputs\":[{\"name\":\"schema\",\"widget-type\":\"schema\",\"widget-attributes\":{\"default-schema\":{\"name\":\"etlSchemaBody\",\"type\":\"record\",\"fields\":[{\"name\":\"message\",\"type\":[\"bytes\",\"null\"]}]},\"schema-default-type\":\"string\",\"property-watch\":\"format\"}}]}",
    "doc.Kafka-batchsink": "# Kafka Sink\n\n\nDescription\n-----------\nKafka sink that allows you to write events into CSV or JSON to kafka.\nPlugin has the capability to push the data to a Kafka topic. It can also be\nconfigured to partition events being written to kafka based on a configurable key. \nThe sink can also be configured to operate in sync or async mode and apply different\ncompression types to events. Kafka sink is compatible with Kafka 0.9 and 0.10\n\n\nConfiguration\n-------------\n**referenceName:** This will be used to uniquely identify this sink for lineage, annotating metadata, etc.\n\n**brokers:** List of Kafka brokers specified in host1:port1,host2:port2 form.\n\n**topic:** The Kafka topic to write to.\n\n**async:** Specifies whether writing the events to broker is *Asynchronous* or *Synchronous*.\n\n**compressionType** Compression type to be applied on message. It can be none, gzip or snappy. Default value is none\n\n**format:** Specifies the format of the event published to Kafka. It can be csv or json. Defualt value is csv.\n\n**kafkaProperties** Specifies additional kafka producer properties like acks, client.id as key and value pair.\n\n**key:** Specifies the input field that should be used as the key for the event published into Kafka. \nIt will use String partitioner to determine kafka event should go to which partition. Key field should be of type string.\n\n**principal** The kerberos principal used for the source when kerberos security is enabled for kafka.\n \n**keytabLocation** The keytab location for the kerberos principal when kerberos security is enabled for kafka.\n\n\nExample\n-------\nThis example writes structured record to kafka topic 'alarm' in asynchronous manner \nusing compression type 'gzip'. The written events will be written in csv format \nto kafka running at localhost. The Kafka partition will be decided based on the provided key 'ts'.\nAdditional properties like number of acknowledgements and client id can also be provided.\n\n\n    {\n        \"name\": \"Kafka\",\n        \"type\": \"batchsink\",\n        \"properties\": {\n            \"referenceName\": \"Kafka\",\n            \"brokers\": \"localhost:9092\",\n            \"topic\": \"alarm\",\n            \"async\": \"FALSE\",\n            \"compressionType\": \"gzip\",\n            \"format\": \"CSV\",\n            \"kafkaProperties\": \"acks:2,client.id:myclient\",\n            \"key\": \"message\"\n        }\n    }",
    "doc.Kafka-batchsource": "# Kafka Batch Source\n\n\nDescription\n-----------\nKafka batch source. Emits the record from kafka. It will emit a record based on the schema and format \nyou use, or if no schema or format is specified, the message payload will be emitted. The source will \nremember the offset it read last run and continue from that offset for the next run.\nThe Kafka batch source supports providing additional kafka properties for the kafka consumer, \nreading from kerberos-enabled kafka and limiting the number of records read\n\nUse Case\n--------\nThis source is used whenever you want to read from Kafka. For example, you may want to read messages\nfrom Kafka and write them to a Table.\n\n\nProperties\n----------\n**referenceName:** This will be used to uniquely identify this source for lineage, annotating metadata, etc.\n\n**kafkaBrokers:** List of Kafka brokers specified in host1:port1,host2:port2 form. (Macro-enabled)\n\n**topic:** The Kafka topic to read from. (Macro-enabled)\n\n**tableName:** Optional table name to track the latest offset we read from kafka. It is recommended to name it same as the \npipeline name to avoid conflict on table names. By default it will be the topic name. (Macro-enabled)\n\n**partitions:** List of topic partitions to read from. If not specified, all partitions will be read. (Macro-enabled)\n\n**initialPartitionOffsets:** The initial offset for each topic partition. This offset will only be used for the \nfirst run of the pipeline. Any subsequent run will read from the latest offset from previous run. \nOffsets are inclusive. If an offset of 5 is used, the message at offset 5 will be read. (Macro-enabled)\n\n**schema:** Output schema of the source. If you would like the output records to contain a field with the\nKafka message key, the schema must include a field of type bytes or nullable bytes, and you must set the\nkeyField property to that field's name. Similarly, if you would like the output records to contain a field with\nthe timestamp of when the record was read, the schema must include a field of type long or nullable long, and you\nmust set the timeField property to that field's name. Any field that is not the keyField, partitionField and keyField\n will be used in conjuction with the format to parse Kafka message payloads.\n \n**maxNumberRecords** The maximum of messages the source will read from each topic partition. \nIf the current topic partition does not have this number of messages, the source will read to the latest offset. \nNote that this is an estimation, the acutal number of messages the source read may be smaller than this number. \n\n**principal** The kerberos principal used for the source when kerberos security is enabled for kafka.\n \n**keytabLocation** The keytab location for the kerberos principal when kerberos security is enabled for kafka.\n\n**kafkaProperties** Additional kafka consumer properties to set.\n\n**format:** Optional format of the Kafka event message. Any format supported by CDAP is supported.\nFor example, a value of 'csv' will attempt to parse Kafka payloads as comma-separated values.\nIf no format is given, Kafka message payloads will be treated as bytes.\n\n**keyField:** Optional name of the field containing the message key.\nIf this is not set, no key field will be added to output records.\nIf set, this field must be present in the schema property and must be bytes.\n\n**partitionField:** Optional name of the field containing the partition the message was read from.\nIf this is not set, no partition field will be added to output records.\nIf set, this field must be present in the schema property and must be an int.\n\n**offsetField:** Optional name of the field containing the partition offset the message was read from.\nIf this is not set, no offset field will be added to output records.\nIf set, this field must be present in the schema property and must be a long.\n\n\nExample\n-------\nThis example reads from the 'purchases' topic of a Kafka instance running\non brokers host1.example.com:9092 and host2.example.com:9092. The source will add\na field named 'key' which will have the message key in it. It parses the Kafka messages \nusing the 'csv' format with 'user', 'item', 'count', and 'price' as the message schema.\n\n    {\n        \"name\": \"Kafka\",\n        \"type\": \"streamingsource\",\n        \"properties\": {\n            \"topics\": \"purchases\",\n            \"brokers\": \"host1.example.com:9092,host2.example.com:9092\",\n            \"format\": \"csv\",\n            \"keyField\": \"key\",\n            \"schema\": \"{\n                \\\"type\\\":\\\"record\\\",\n                \\\"name\\\":\\\"purchase\\\",\n                \\\"fields\\\":[\n                    {\\\"name\\\":\\\"key\\\",\\\"type\\\":\\\"bytes\\\"},\n                    {\\\"name\\\":\\\"user\\\",\\\"type\\\":\\\"string\\\"},\n                    {\\\"name\\\":\\\"item\\\",\\\"type\\\":\\\"string\\\"},\n                    {\\\"name\\\":\\\"count\\\",\\\"type\\\":\\\"int\\\"},\n                    {\\\"name\\\":\\\"price\\\",\\\"type\\\":\\\"double\\\"}\n                ]\n            }\"\n        }\n    }\n\nFor each Kafka message read, it will output a record with the schema:\n\n    +================================+\n    | field name  | type             |\n    +================================+\n    | key         | bytes            |\n    | user        | string           |\n    | item        | string           |\n    | count       | int              |\n    | price       | double           |\n    +================================+\n    ",
    "doc.KAFKABATCHSOURCE": "[![Build Status](https://travis-ci.org/hydrator/kafka-plugins.svg?branch=master)](https://travis-ci.org/hydrator/kafka-plugins) [![License](https://img.shields.io/badge/License-Apache%202.0-blue.svg)](https://opensource.org/licenses/Apache-2.0)\n\nKafka Batch Source\n===========\n\nKafka batch source that emits a records with user specified schema.\n\n<img align=\"center\" src=\"kafka-batch-source-plugins-config.png\"  width=\"400\" alt=\"plugin configuration\" />\n\nUsage Notes\n-----------\n\nKafka Batch Source can be used to read events from a kafka topic. It uses kafka consumer [0.9.1 apis](https://kafka.apache.org/090/documentation.html) to read events from a kafka topic. The Kafka Batch Source supports providing additional kafka properties for the kafka consumer, reading from kerberos-enabled kafka and limiting the number of records read. Kafka Batch Source converts incoming kafka events into cdap structured records which then can be used for further transformations. \n\nThe source will read from the earliest available offset or the initial offset that specified in the config for the first run, remember the last offset it read last run and continue from that offset for the next run. \n\nPlugin Configuration\n---------------------\n\n| Configuration | Required | Default | Description |\n| :------------ | :------: | :----- | :---------- |\n| **Kafka Brokers** | **Y** | N/A | List of Kafka brokers specified in host1:port1,host2:port2 form. |\n| **Kafka Topic** | **Y** | N/A | The Kafka topic to read from. |\n| **Offset Table Name** | **Y** | N/A | Optional table name to track the latest offset we read from kafka. It is recommended to name it same as the pipeline name to avoid conflict on table names. By default it will be the topic name.\n| **Topic Partition** | **N** | N/A | List of topic partitions to read from. If not specified, all partitions will be read.  |\n| **Initial Partition Offsets** | **N** | N/A | The initial offset for each topic partition. If this is not specified, earliest offset will be used. This offset will only be used for the first run of the pipeline. Any subsequent run will read from the latest offset from previous run.  Offsets are inclusive. If an offset of 5 is used, the message at offset 5 will be read. |\n| **Key Field** | **N** | N/A | Optional name of the field containing the message key. If this is not set, no key field will be added to output records. If set, this field must be present in the schema property and must be bytes. |\n| **Partition Field** | **N** | N/A | Optional name of the field containing the partition the message was read from. If this is not set, no partition field will be added to output records. If set, this field must be present in the schema property and must be an int. |\n| **Offset Field** | **N** | N/A | Optional name of the field containing the partition offset the message was read from. If this is not set, no offset field will be added to output records. If set, this field must be present in the schema property and must be a long. |\n| **Max Number Records** | **N** | N/A | The maximum of messages the source will read from each topic partition. If the current topic partition does not have this number of messages, the source will read to the latest offset. Note that this is an estimation, the acutal number of messages the source read may be smaller than this number. |\n| **Kerberos Principal** | **N** | N/A | The kerberos principal used for the source when kerberos security is enabled for kafka. |\n| **Keytab Location** | **N** | N/A | The keytab location for the kerberos principal when kerberos security is enabled for kafka. |\n| **Additional Kafka Consumer Properties** | **N** | N/A | Additional kafka consumer properties to set. |\n| **Format** | **N** | N/A | Optional format of the Kafka event message. Any format supported by CDAP is supported. For example, a value of 'csv' will attempt to parse Kafka payloads as comma-separated values. If no format is given, Kafka message payloads will be treated as bytes. |\n\n\nBuild\n-----\nTo build this plugin:\n\n```\n   mvn clean package\n```    \n\nThe build will create a .jar and .json file under the ``target`` directory.\nThese files can be used to deploy your plugins.\n\nDeployment\n----------\nYou can deploy your plugins using the CDAP CLI:\n\n    > load artifact <target/kafka-plugins-<version>.jar config-file <target/kafka-plugins<version>.json>\n\nFor example, if your artifact is named 'kafka-plugins-<version>':\n\n    > load artifact target/kafka-plugins-<version>.jar config-file target/kafka-plugins-<version>.json\n    \n## Mailing Lists\n\nCDAP User Group and Development Discussions:\n\n* `cdap-user@googlegroups.com <https://groups.google.com/d/forum/cdap-user>`\n\nThe *cdap-user* mailing list is primarily for users using the product to develop\napplications or building plugins for appplications. You can expect questions from \nusers, release announcements, and any other discussions that we think will be helpful \nto the users.\n\n## License and Trademarks\n\nCopyright © 2017 Cask Data, Inc.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except\nin compliance with the License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the \nLicense is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, \neither express or implied. See the License for the specific language governing permissions \nand limitations under the License.\n\nCask is a trademark of Cask Data, Inc. All rights reserved.\n\nApache, Apache HBase, and HBase are trademarks of The Apache Software Foundation. Used with\npermission. No endorsement by The Apache Software Foundation is implied by the use of these marks.      \n",
    "doc.KAFKAWRITER-SINK": "[![Build Status](https://travis-ci.org/hydrator/kafka-plugins.svg?branch=master)](https://travis-ci.org/hydrator/kafka-plugins) [![License](https://img.shields.io/badge/License-Apache%202.0-blue.svg)](https://opensource.org/licenses/Apache-2.0)\n\nKafka Sink\n==========\n\nKafka sink that allows you to write events into CSV or JSON to kafka. Plugin has the capability to push the data to one or more Kafka topics. \nIt can use one of the field values from input to partition the data on topic. The sink can also be configured to operate in either sync or async mode. \nThe sink also allows you to write events into kerberos-enabled kafka.\n\n<img align=\"center\" src=\"kafka-sink-plugin-config.png\"  width=\"400\" alt=\"plugin configuration\" />\n\nUsage Notes\n-----------\n\nKafka sink emits events in realtime to configured kafka topic and partition. It uses kafka producer [0.8.2 apis](https://kafka.apache.org/082/javadoc/index.html?org/apache/kafka/clients/producer/KafkaProducer.html) to write events into kafka. \n\nThis sink can be configured to operate in synchronous or asynchronous mode. In synchronous mode, each event will be sent to the broker synchronously on the thread that calls it. This is not sufficient on most of the high volume environments. \nIn async mode, the kafka producer will batch together all the kafka events for greater throughput. But that makes it open for the possibility of dropping unsent events in case of client machine failure. Since kafka producer by default uses synchronous mode, this sink also uses Synchronous producer by default.\n\nIt uses String partitioner and String serializer for key and value to write events to kafka. Optionally if kafka key is provided, producer will use that key to partition events accross multiple partitions in a given topic. This sink also allows compression configuration. By default compression is none.\n\nKafka producer can be tuned using many properties as shown [here](https://kafka.apache.org/082/javadoc/org/apache/kafka/clients/producer/ProducerConfig.html). This sink allows user to configure any property supported by kafka 0.8.2 Producer.\n\n\nPlugin Configuration\n---------------------\n\n| Configuration | Required | Default | Description |\n| :------------ | :------: | :----- | :---------- |\n| **Kafka Brokers** | **Y** | N/A | List of Kafka brokers specified in host1:port1,host2:port2 form. |\n| **Kafka Topic** | **Y** | N/A | The Kafka topic to write to. This should be a valid kafka topic string. Kafka topic should already exist. |\n| **Is Async** | **Y** | False | Specifies whether writing the events to broker is *Asynchronous* or *Synchronous*.  |\n| **Compression Type** | **Y** | none | This configuration specifies the format of the event published to Kafka. |\n| **Kerberos Principal** | **N** | N/A | The kerberos principal used for the source when kerberos security is enabled for kafka. |\n| **Keytab Location** | **N** | N/A | The keytab location for the kerberos principal when kerberos security is enabled for kafka. |\n| **Additional Kafka Producer Properties** | **N** | N/A | Specifies additional kafka producer properties like acks, client.id as key and value pair. |\n| **Message Format** | **Y** | CSV | This configuration specifies serialization format of the event published to Kafka. |\n| **Message Key Field** | **N** | N/A | This configuration specifies the input field that should be used as the key for the event published into Kafka. This field will be used to partition kafka events across multiple partitions of a topic. Key field should be of type string. |\n\n\nBuild\n-----\nTo build this plugin:\n\n```\n   mvn clean package\n```    \n\nThe build will create a .jar and .json file under the ``target`` directory.\nThese files can be used to deploy your plugins.\n\nDeployment\n----------\nYou can deploy your plugins using the CDAP CLI:\n\n    > load artifact <target/kafka-plugins-<version>.jar config-file <target/kafka-plugins<version>.json>\n\nFor example, if your artifact is named 'kafka-plugins-<version>':\n\n    > load artifact target/kafka-plugins-<version>.jar config-file target/kafka-plugins-<version>.json\n    \n## Mailing Lists\n\nCDAP User Group and Development Discussions:\n\n* `cdap-user@googlegroups.com <https://groups.google.com/d/forum/cdap-user>`\n\nThe *cdap-user* mailing list is primarily for users using the product to develop\napplications or building plugins for appplications. You can expect questions from \nusers, release announcements, and any other discussions that we think will be helpful \nto the users.\n\n## License and Trademarks\n\nCopyright © 2017 Cask Data, Inc.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except\nin compliance with the License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the \nLicense is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, \neither express or implied. See the License for the specific language governing permissions \nand limitations under the License.\n\nCask is a trademark of Cask Data, Inc. All rights reserved.\n\nApache, Apache HBase, and HBase are trademarks of The Apache Software Foundation. Used with\npermission. No endorsement by The Apache Software Foundation is implied by the use of these marks.      \n"
  }
}
