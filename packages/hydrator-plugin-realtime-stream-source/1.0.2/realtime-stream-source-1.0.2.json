{
  "parents": [
    "system:cdap-etl-batch[4.1.0,6.0.0-SNAPSHOT)",
    "system:cdap-data-pipeline[4.1.0,6.0.0-SNAPSHOT)",
    "system:cdap-data-streams[4.1.0,6.0.0-SNAPSHOT)"
  ],
  "properties": {
    "widgets.Stream-streamingsource": "{\"metadata\":{\"spec-version\":\"1.3\"},\"configuration-groups\":[{\"label\":\"Stream Configuration\",\"properties\":[{\"widget-type\":\"stream-selector\",\"label\":\"Stream Name\",\"name\":\"name\"},{\"widget-type\":\"textbox\",\"label\":\"Data Field Name\",\"name\":\"body.field\"},{\"widget-type\":\"textbox\",\"label\":\"Header Field Name\",\"name\":\"headers.field\"}]},{\"label\":\"Format\",\"properties\":[{\"widget-type\":\"select\",\"label\":\"Format\",\"name\":\"format\",\"widget-attributes\":{\"values\":[\"avro\",\"binary\",\"clf\",\"csv\",\"grok\",\"syslog\",\"text\",\"tsv\"],\"default\":\"text\"}}]}],\"outputs\":[{\"name\":\"schema\",\"widget-type\":\"schema\",\"widget-attributes\":{\"default-schema\":{\"name\":\"etlSchemaBody\",\"type\":\"record\",\"fields\":[{\"name\":\"body\",\"type\":\"string\"}]},\"schema-default-type\":\"string\",\"property-watch\":\"format\"}}],\"jump-config\":{\"streams\":[{\"ref-property-name\":\"name\"}]}}",
    "doc.Stream-streamingsource": "# Stream Realtime spark streaming Source\n\n\nDescription\n-----------\nReads realtime data from a stream. If the format and schema of the stream are known,\nthey can be specified as well. The source will return a record for each stream event it reads.\nRecords will always contain a 'ts' field of type 'long' that contains the timestamp of the event,\nas well as a 'headers' field of type 'map<string, string>' that contains the headers for\nthe event. Other fields output records are determined by the configured format and schema.\n\nProperties\n----------\n**name:** Name of the stream. Must be a valid stream name. If the stream does not exist,\nit will be created. (Macro-enabled)\n\n**format:** Optional format of the stream. Any format supported by CDAP is also supported.\nFor example, a value of 'csv' will attempt to parse stream events as comma separated\nvalues. If no format is given, event bodies will be treated as bytes, resulting in a\nthree-field schema: 'ts' of type long, 'headers' of type map of string to string, and 'body' of\ntype bytes.\n\n**schema:** Optional schema for the body of stream events. Schema is used in conjunction\nwith format to parse stream events. Some formats like the avro format require schema,\nwhile others do not. The schema given is for the body of the stream, so the final schema\nof records output by the source will contain an additional field named 'ts' for the\ntimestamp and a field named 'headers' for the headers as the first and second fields of\nthe schema.\n\n\nExample\n-------\nThis example reads from a stream named 'devices':\n\n    {\n        \"name\": \"Stream\",\n        \"type\": \"batchsource\",\n        \"properties\": {\n            \"name\": \"devices\",\n            \"format\": \"csv\"\n        }\n    }\n\nThe stream contents will be parsed as 'csv' (Comma Separated Values), which will output\nrecords with this schema:\n\n    +======================================+\n    | field name     | type                |\n    +======================================+\n    | ts             | long                |\n    | headers        | map<string, string> |\n    | device_id      | nullable string     |\n    | device_name    | nullable string     |\n    | user           | nullable string     |\n    | request_time   | nullable string     |\n    | request        | nullable string     |\n    | status         | nullable string     |\n    | content_length | nullable string     |\n    +======================================+\n\nThe 'ts' and 'headers' fields will be always be present regardless of the stream format.\nAll other fields in this example come from the schema defined by the user.\n"
  }
}